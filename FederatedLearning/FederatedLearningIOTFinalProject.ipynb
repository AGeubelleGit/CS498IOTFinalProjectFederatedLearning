{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.simonwenkel.com/2019/07/20/introduction-to-pysyft.html\n",
    "\n",
    "# How to load a federated data set: https://github.com/OpenMined/PySyft/blob/syft_0.2.x/examples/tutorials/advanced/Federated%20Dataset.ipynb?short_path=dddb5e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from syft.workers.websocket_client import WebsocketClientWorker\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "import syft as sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 20\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 5\n",
    "        self.save_model = False\n",
    "\n",
    "args = Arguments()\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused in final version, but this allows your workers to be virtual and on the same device (good for testing)\n",
    "worker1 = sy.VirtualWorker(hook, id=\"worker1\")\n",
    "worker2 = sy.VirtualWorker(hook, id=\"worker2\")\n",
    "worker3 = sy.VirtualWorker(hook, id=\"worker3\")\n",
    "worker4 = sy.VirtualWorker(hook, id=\"worker4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to remote workers via ip address\n",
    "kwargs_websocket = {\"host\": \"localhost\", \"hook\": hook}\n",
    "# kwargs_websocket = {\"host\": \"192.168.10.49\", \"hook\": hook}\n",
    "alice = WebsocketClientWorker(id='alice', port=8779, **kwargs_websocket)\n",
    "bob = WebsocketClientWorker(id='bob', port=8778, **kwargs_websocket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to manipulate input data\n",
    "def split_array(a, chunk_size=80):\n",
    "    d, N = a.shape\n",
    "    num_splits = d // chunk_size\n",
    "    b = a[:(num_splits*chunk_size),:]\n",
    "    out = np.vsplit(b, num_splits)\n",
    "    return out\n",
    "\n",
    "def normalize_array(a):\n",
    "    means = np.mean(a,axis=0)\n",
    "    a = a - means\n",
    "    stds = np.std(a, axis=0)\n",
    "    a = a / stds\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_labels = ['X','Y','Z']\n",
    "activity_labels = ['Climb_stairs', 'Pour_water', 'Drink_glass']\n",
    "\n",
    "# Load Data\n",
    "X = list()\n",
    "Y = list()\n",
    "for activity in activity_labels:\n",
    "    for file in os.listdir('data/HMP_Dataset/'+activity):\n",
    "        filepath = 'data/HMP_Dataset/'+activity+'/'+file\n",
    "        arr = np.loadtxt(filepath,  delimiter=\" \")\n",
    "        norm_arr = normalize_array(arr)\n",
    "        split_arr = split_array(norm_arr, chunk_size=80)\n",
    "        X.extend(split_arr)\n",
    "        Y.extend([activity]*len(split_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess/Manipulate data, splitting into train/test \n",
    "X_arr = np.array(X)\n",
    "Y_arr = np.array(Y)\n",
    "\n",
    "num_classes = len(activity_labels)\n",
    "N, time_size, axes = X_arr.shape\n",
    "X_arr = X_arr.reshape((N, time_size*axes))\n",
    "\n",
    "# Encode labesl into integers\n",
    "le = preprocessing.LabelEncoder()\n",
    "Y_arr = le.fit_transform(Y_arr).reshape((len(Y_arr),1))\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "Y_arr = enc.fit(Y_arr).transform(Y_arr).toarray()\n",
    "\n",
    "# Train-test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_arr, Y_arr, test_size=0.33, random_state=42)\n",
    "\n",
    "# Torch tensors\n",
    "X_test_torch = torch.from_numpy(X_test).float()\n",
    "y_test_torch = torch.from_numpy(y_test).float()\n",
    "tensorDatasetTest = TensorDataset(X_test_torch, y_test_torch)\n",
    "test_loader = DataLoader(tensorDatasetTest, batch_size=args.test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each datapoint to a device in a federated manner\n",
    "base=sy.BaseDataset(torch.from_numpy(X_train).float(),torch.from_numpy(y_train).float())\n",
    "base_federated=base.federate((alice, bob))\n",
    "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
    "                         base_federated,batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup neural network layers\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(240, 100)\n",
    "        self.fc2 = nn.Linear(100, 32)\n",
    "        self.fc3 = nn.Linear(32, 24)\n",
    "        self.fc4 = nn.Linear(24, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train one epoch of the model\n",
    "def train(args, model, device, federated_train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "#     For each batch of data, send to the device, have the device calculate a loss and update weights, retrieve model\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "        model.send(data.location) # <-- NEW: send the model to the right location\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.mse_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get() # <-- NEW: get the model back\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get()  # <-- NEW: get the loss back\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(federated_train_loader), loss.item()))\n",
    "\n",
    "#  Test the model's accuracy and loss\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.mse_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.argmax(1, keepdim=True)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/960 (0%)]\tLoss: 2.255010\n",
      "Train Epoch: 1 [100/960 (10%)]\tLoss: 2.246036\n",
      "Train Epoch: 1 [200/960 (21%)]\tLoss: 2.256964\n",
      "Train Epoch: 1 [300/960 (31%)]\tLoss: 2.245172\n",
      "Train Epoch: 1 [400/960 (42%)]\tLoss: 2.253455\n",
      "Train Epoch: 1 [500/960 (52%)]\tLoss: 2.248050\n",
      "Train Epoch: 1 [600/960 (62%)]\tLoss: 2.248375\n",
      "Train Epoch: 1 [700/960 (73%)]\tLoss: 2.254012\n",
      "Train Epoch: 1 [800/960 (83%)]\tLoss: 2.251063\n",
      "Train Epoch: 1 [900/960 (94%)]\tLoss: 2.235434\n",
      "\n",
      "Test set: Average loss: 6.7547, Accuracy: 312/469 (67%)\n",
      "\n",
      "Train Epoch: 2 [0/960 (0%)]\tLoss: 2.257907\n",
      "Train Epoch: 2 [100/960 (10%)]\tLoss: 2.252656\n",
      "Train Epoch: 2 [200/960 (21%)]\tLoss: 2.250963\n",
      "Train Epoch: 2 [300/960 (31%)]\tLoss: 2.236250\n",
      "Train Epoch: 2 [400/960 (42%)]\tLoss: 2.245930\n",
      "Train Epoch: 2 [500/960 (52%)]\tLoss: 2.247042\n",
      "Train Epoch: 2 [600/960 (62%)]\tLoss: 2.247000\n",
      "Train Epoch: 2 [700/960 (73%)]\tLoss: 2.246808\n",
      "Train Epoch: 2 [800/960 (83%)]\tLoss: 2.249908\n",
      "Train Epoch: 2 [900/960 (94%)]\tLoss: 2.238326\n",
      "\n",
      "Test set: Average loss: 6.7505, Accuracy: 318/469 (68%)\n",
      "\n",
      "Train Epoch: 3 [0/960 (0%)]\tLoss: 2.233934\n",
      "Train Epoch: 3 [100/960 (10%)]\tLoss: 2.250870\n",
      "Train Epoch: 3 [200/960 (21%)]\tLoss: 2.251521\n",
      "Train Epoch: 3 [300/960 (31%)]\tLoss: 2.250057\n",
      "Train Epoch: 3 [400/960 (42%)]\tLoss: 2.239021\n",
      "Train Epoch: 3 [500/960 (52%)]\tLoss: 2.255930\n",
      "Train Epoch: 3 [600/960 (62%)]\tLoss: 2.236906\n",
      "Train Epoch: 3 [700/960 (73%)]\tLoss: 2.252584\n",
      "Train Epoch: 3 [800/960 (83%)]\tLoss: 2.243060\n",
      "Train Epoch: 3 [900/960 (94%)]\tLoss: 2.243782\n",
      "\n",
      "Test set: Average loss: 6.7460, Accuracy: 324/469 (69%)\n",
      "\n",
      "Train Epoch: 4 [0/960 (0%)]\tLoss: 2.248462\n",
      "Train Epoch: 4 [100/960 (10%)]\tLoss: 2.263359\n",
      "Train Epoch: 4 [200/960 (21%)]\tLoss: 2.246832\n",
      "Train Epoch: 4 [300/960 (31%)]\tLoss: 2.247727\n",
      "Train Epoch: 4 [400/960 (42%)]\tLoss: 2.250294\n",
      "Train Epoch: 4 [500/960 (52%)]\tLoss: 2.247808\n",
      "Train Epoch: 4 [600/960 (62%)]\tLoss: 2.239395\n",
      "Train Epoch: 4 [700/960 (73%)]\tLoss: 2.240007\n",
      "Train Epoch: 4 [800/960 (83%)]\tLoss: 2.237484\n",
      "Train Epoch: 4 [900/960 (94%)]\tLoss: 2.241417\n",
      "\n",
      "Test set: Average loss: 6.7419, Accuracy: 329/469 (70%)\n",
      "\n",
      "Train Epoch: 5 [0/960 (0%)]\tLoss: 2.242222\n",
      "Train Epoch: 5 [100/960 (10%)]\tLoss: 2.249130\n",
      "Train Epoch: 5 [200/960 (21%)]\tLoss: 2.243279\n",
      "Train Epoch: 5 [300/960 (31%)]\tLoss: 2.247354\n",
      "Train Epoch: 5 [400/960 (42%)]\tLoss: 2.240055\n",
      "Train Epoch: 5 [500/960 (52%)]\tLoss: 2.240239\n",
      "Train Epoch: 5 [600/960 (62%)]\tLoss: 2.231311\n",
      "Train Epoch: 5 [700/960 (73%)]\tLoss: 2.247148\n",
      "Train Epoch: 5 [800/960 (83%)]\tLoss: 2.238980\n",
      "Train Epoch: 5 [900/960 (94%)]\tLoss: 2.246559\n",
      "\n",
      "Test set: Average loss: 6.7377, Accuracy: 338/469 (72%)\n",
      "\n",
      "Train Epoch: 6 [0/960 (0%)]\tLoss: 2.240760\n",
      "Train Epoch: 6 [100/960 (10%)]\tLoss: 2.251406\n",
      "Train Epoch: 6 [200/960 (21%)]\tLoss: 2.238965\n",
      "Train Epoch: 6 [300/960 (31%)]\tLoss: 2.236431\n",
      "Train Epoch: 6 [400/960 (42%)]\tLoss: 2.242035\n",
      "Train Epoch: 6 [500/960 (52%)]\tLoss: 2.238914\n",
      "Train Epoch: 6 [600/960 (62%)]\tLoss: 2.234984\n",
      "Train Epoch: 6 [700/960 (73%)]\tLoss: 2.241923\n",
      "Train Epoch: 6 [800/960 (83%)]\tLoss: 2.237063\n",
      "Train Epoch: 6 [900/960 (94%)]\tLoss: 2.239879\n",
      "\n",
      "Test set: Average loss: 6.7337, Accuracy: 346/469 (74%)\n",
      "\n",
      "Train Epoch: 7 [0/960 (0%)]\tLoss: 2.242125\n",
      "Train Epoch: 7 [100/960 (10%)]\tLoss: 2.242426\n",
      "Train Epoch: 7 [200/960 (21%)]\tLoss: 2.227906\n",
      "Train Epoch: 7 [300/960 (31%)]\tLoss: 2.236050\n",
      "Train Epoch: 7 [400/960 (42%)]\tLoss: 2.239890\n",
      "Train Epoch: 7 [500/960 (52%)]\tLoss: 2.239407\n",
      "Train Epoch: 7 [600/960 (62%)]\tLoss: 2.246869\n",
      "Train Epoch: 7 [700/960 (73%)]\tLoss: 2.234539\n",
      "Train Epoch: 7 [800/960 (83%)]\tLoss: 2.239506\n",
      "Train Epoch: 7 [900/960 (94%)]\tLoss: 2.233121\n",
      "\n",
      "Test set: Average loss: 6.7297, Accuracy: 348/469 (74%)\n",
      "\n",
      "Train Epoch: 8 [0/960 (0%)]\tLoss: 2.232604\n",
      "Train Epoch: 8 [100/960 (10%)]\tLoss: 2.237673\n",
      "Train Epoch: 8 [200/960 (21%)]\tLoss: 2.247162\n",
      "Train Epoch: 8 [300/960 (31%)]\tLoss: 2.243686\n",
      "Train Epoch: 8 [400/960 (42%)]\tLoss: 2.233707\n",
      "Train Epoch: 8 [500/960 (52%)]\tLoss: 2.242869\n",
      "Train Epoch: 8 [600/960 (62%)]\tLoss: 2.228810\n",
      "Train Epoch: 8 [700/960 (73%)]\tLoss: 2.228396\n",
      "Train Epoch: 8 [800/960 (83%)]\tLoss: 2.240754\n",
      "Train Epoch: 8 [900/960 (94%)]\tLoss: 2.239585\n",
      "\n",
      "Test set: Average loss: 6.7260, Accuracy: 358/469 (76%)\n",
      "\n",
      "Train Epoch: 9 [0/960 (0%)]\tLoss: 2.218675\n",
      "Train Epoch: 9 [100/960 (10%)]\tLoss: 2.231606\n",
      "Train Epoch: 9 [200/960 (21%)]\tLoss: 2.238228\n",
      "Train Epoch: 9 [300/960 (31%)]\tLoss: 2.250904\n",
      "Train Epoch: 9 [400/960 (42%)]\tLoss: 2.239537\n",
      "Train Epoch: 9 [500/960 (52%)]\tLoss: 2.239292\n",
      "Train Epoch: 9 [600/960 (62%)]\tLoss: 2.240718\n",
      "Train Epoch: 9 [700/960 (73%)]\tLoss: 2.234466\n",
      "Train Epoch: 9 [800/960 (83%)]\tLoss: 2.236651\n",
      "Train Epoch: 9 [900/960 (94%)]\tLoss: 2.231309\n",
      "\n",
      "Test set: Average loss: 6.7224, Accuracy: 362/469 (77%)\n",
      "\n",
      "Train Epoch: 10 [0/960 (0%)]\tLoss: 2.235317\n",
      "Train Epoch: 10 [100/960 (10%)]\tLoss: 2.242100\n",
      "Train Epoch: 10 [200/960 (21%)]\tLoss: 2.237168\n",
      "Train Epoch: 10 [300/960 (31%)]\tLoss: 2.220068\n",
      "Train Epoch: 10 [400/960 (42%)]\tLoss: 2.220273\n",
      "Train Epoch: 10 [500/960 (52%)]\tLoss: 2.244567\n",
      "Train Epoch: 10 [600/960 (62%)]\tLoss: 2.224522\n",
      "Train Epoch: 10 [700/960 (73%)]\tLoss: 2.231441\n",
      "Train Epoch: 10 [800/960 (83%)]\tLoss: 2.236927\n",
      "Train Epoch: 10 [900/960 (94%)]\tLoss: 2.247403\n",
      "\n",
      "Test set: Average loss: 6.7188, Accuracy: 368/469 (78%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model for a number of epochs\n",
    "# model = Net().to(device)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
    "    test(args, model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracies(model, name):\n",
    "    print(\"Accuracies for \" + name)\n",
    "    class_correct = list(0. for i in range(len(activity_labels)))\n",
    "    class_total = list(0. for i in range(len(activity_labels)))\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, actual = torch.max(labels, 1)\n",
    "            for i, e in enumerate(predicted == actual):\n",
    "                if e == True:\n",
    "                    class_correct[actual[i]] += 1\n",
    "                class_total[actual[i]] += 1\n",
    "\n",
    "    for i in range(len(activity_labels)):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            activity_labels[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for general\n",
      "Accuracy of Climb_stairs : 81 %\n",
      "Accuracy of Pour_water : 74 %\n",
      "Accuracy of Drink_glass : 79 %\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracies(model, \"general\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
